"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.Kafka = exports.KafkaConsume = exports.DBOSKafka = void 0;
const kafkajs_1 = require("kafkajs");
const __1 = require("..");
const __2 = require("..");
const utils_1 = require("../utils");
////////////////////////
/* Kafka Management  */
///////////////////////
class DBOSKafka {
    consumers = [];
    executor = undefined;
    constructor() { }
    async initialize(dbosExecI) {
        this.executor = dbosExecI;
        const regops = this.executor.getRegistrationsFor(this);
        for (const registeredOperation of regops) {
            const ro = registeredOperation.methodConfig;
            if (ro.kafkaTopics) {
                const defaults = registeredOperation.classConfig;
                const method = registeredOperation.methodReg;
                const cname = method.className;
                const mname = method.name;
                if (!method.txnConfig && !method.workflowConfig) {
                    throw new __2.Error.DBOSError(`Error registering method ${cname}.${mname}: A Kafka decorator can only be assigned to a transaction or workflow!`);
                }
                if (!defaults.kafkaConfig) {
                    throw new __2.Error.DBOSError(`Error registering method ${cname}.${mname}: Kafka configuration not found. Does class ${cname} have an @Kafka decorator?`);
                }
                const topics = [];
                if (Array.isArray(ro.kafkaTopics)) {
                    topics.push(...ro.kafkaTopics);
                }
                else if (ro.kafkaTopics) {
                    topics.push(ro.kafkaTopics);
                }
                const kafka = new kafkajs_1.Kafka(defaults.kafkaConfig);
                const consumerConfig = ro.consumerConfig ?? { groupId: `${this.safeGroupName(topics)}` };
                const consumer = kafka.consumer(consumerConfig);
                await consumer.connect();
                // A temporary workaround for https://github.com/tulios/kafkajs/pull/1558 until it gets fixed
                // If topic autocreation is on and you try to subscribe to a nonexistent topic, KafkaJS should retry until the topic is created.
                // However, it has a bug where it won't. Thus, we retry instead.
                const maxRetries = defaults.kafkaConfig.retry ? defaults.kafkaConfig.retry.retries ?? 5 : 5;
                let retryTime = defaults.kafkaConfig.retry ? defaults.kafkaConfig.retry.maxRetryTime ?? 300 : 300;
                const multiplier = defaults.kafkaConfig.retry ? defaults.kafkaConfig.retry.multiplier ?? 2 : 2;
                for (let i = 0; i < maxRetries; i++) {
                    try {
                        await consumer.subscribe({ topics: topics, fromBeginning: true });
                        break;
                    }
                    catch (error) {
                        const e = error;
                        if (e.code === 3 && i + 1 < maxRetries) { // UNKNOWN_TOPIC_OR_PARTITION
                            await (0, utils_1.sleepms)(retryTime);
                            retryTime *= multiplier;
                            continue;
                        }
                        else {
                            throw e;
                        }
                    }
                }
                await consumer.run({
                    eachMessage: async ({ topic, partition, message }) => {
                        // This combination uniquely identifies a message for a given Kafka cluster
                        const workflowUUID = `kafka-unique-id-${topic}-${partition}-${message.offset}`;
                        const wfParams = { workflowUUID: workflowUUID, configuredInstance: null, queueName: ro.queueName };
                        // All operations annotated with Kafka decorators must take in these three arguments
                        const args = [topic, partition, message];
                        // We can only guarantee exactly-once-per-message execution of transactions and workflows.
                        if (method.txnConfig) {
                            // Execute the transaction
                            await this.executor.transaction(method.registeredFunction, wfParams, ...args);
                        }
                        else if (method.workflowConfig) {
                            // Safely start the workflow
                            await this.executor.workflow(method.registeredFunction, wfParams, ...args);
                        }
                    },
                });
                this.consumers.push(consumer);
            }
        }
    }
    async destroy() {
        for (const consumer of this.consumers) {
            await consumer.disconnect();
        }
    }
    safeGroupName(topics) {
        const safeGroupIdPart = topics
            .map(r => r.toString())
            .map(r => r.replaceAll(/[^a-zA-Z0-9\\-]/g, ''))
            .join('-');
        return `dbos-kafka-group-${safeGroupIdPart}`.slice(0, 255);
    }
    logRegisteredEndpoints() {
        if (!this.executor)
            return;
        const logger = this.executor.logger;
        logger.info("Kafka endpoints supported:");
        const regops = this.executor.getRegistrationsFor(this);
        regops.forEach((registeredOperation) => {
            const ro = registeredOperation.methodConfig;
            if (ro.kafkaTopics) {
                const cname = registeredOperation.methodReg.className;
                const mname = registeredOperation.methodReg.name;
                if (Array.isArray(ro.kafkaTopics)) {
                    ro.kafkaTopics.forEach(kafkaTopic => {
                        logger.info(`    ${kafkaTopic} -> ${cname}.${mname}`);
                    });
                }
                else {
                    logger.info(`    ${ro.kafkaTopics} -> ${cname}.${mname}`);
                }
            }
        });
    }
}
exports.DBOSKafka = DBOSKafka;
/////////////////////////////
/* Kafka Method Decorators */
/////////////////////////////
let kafkaInst = undefined;
function KafkaConsume(topics, consumerConfig, queueName) {
    function kafkadec(target, propertyKey, inDescriptor) {
        if (!kafkaInst)
            kafkaInst = new DBOSKafka();
        const { descriptor, receiverInfo } = (0, __1.associateMethodWithEventReceiver)(kafkaInst, target, propertyKey, inDescriptor);
        const kafkaRegistration = receiverInfo;
        kafkaRegistration.kafkaTopics = topics;
        kafkaRegistration.consumerConfig = consumerConfig;
        kafkaRegistration.queueName = queueName;
        return descriptor;
    }
    return kafkadec;
}
exports.KafkaConsume = KafkaConsume;
function Kafka(kafkaConfig) {
    function clsdec(ctor) {
        if (!kafkaInst)
            kafkaInst = new DBOSKafka();
        const kafkaInfo = (0, __1.associateClassWithEventReceiver)(kafkaInst, ctor);
        kafkaInfo.kafkaConfig = kafkaConfig;
    }
    return clsdec;
}
exports.Kafka = Kafka;
//# sourceMappingURL=kafka.js.map