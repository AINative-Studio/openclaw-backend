"use strict";
/* eslint-disable @typescript-eslint/no-explicit-any */
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.PostgresSystemDatabase = exports.migrateSystemDatabase = void 0;
const serialize_error_1 = require("serialize-error");
const dbos_executor_1 = require("./dbos-executor");
const pg_1 = require("pg");
const error_1 = require("./error");
const workflow_1 = require("./workflow");
const utils_1 = require("./utils");
const knex_1 = __importDefault(require("knex"));
const path_1 = __importDefault(require("path"));
async function migrateSystemDatabase(systemPoolConfig) {
    const migrationsDirectory = path_1.default.join((0, utils_1.findPackageRoot)(__dirname), 'migrations');
    const knexConfig = {
        client: 'pg',
        connection: systemPoolConfig,
        migrations: {
            directory: migrationsDirectory,
            tableName: 'knex_migrations'
        }
    };
    const knexDB = (0, knex_1.default)(knexConfig);
    try {
        await knexDB.migrate.latest();
    }
    finally {
        await knexDB.destroy();
    }
}
exports.migrateSystemDatabase = migrateSystemDatabase;
class PostgresSystemDatabase {
    pgPoolConfig;
    systemDatabaseName;
    logger;
    pool;
    systemPoolConfig;
    knexDB;
    notificationsClient = null;
    notificationsMap = {};
    workflowEventsMap = {};
    workflowStatusBuffer = new Map();
    workflowInputsBuffer = new Map();
    flushBatchSize = 100;
    static connectionTimeoutMillis = 10000; // 10 second timeout
    constructor(pgPoolConfig, systemDatabaseName, logger) {
        this.pgPoolConfig = pgPoolConfig;
        this.systemDatabaseName = systemDatabaseName;
        this.logger = logger;
        this.systemPoolConfig = { ...pgPoolConfig };
        this.systemPoolConfig.database = systemDatabaseName;
        this.systemPoolConfig.connectionTimeoutMillis = PostgresSystemDatabase.connectionTimeoutMillis;
        this.pool = new pg_1.Pool(this.systemPoolConfig);
        const knexConfig = {
            client: 'pg',
            connection: this.systemPoolConfig,
            pool: {
                max: 2
            }
        };
        this.knexDB = (0, knex_1.default)(knexConfig);
    }
    async init() {
        const pgSystemClient = new pg_1.Client(this.pgPoolConfig);
        await pgSystemClient.connect();
        // Create the system database and load tables.
        const dbExists = await pgSystemClient.query(`SELECT EXISTS (SELECT FROM pg_database WHERE datname = '${this.systemDatabaseName}')`);
        if (!dbExists.rows[0].exists) {
            // Create the DBOS system database.
            await pgSystemClient.query(`CREATE DATABASE "${this.systemDatabaseName}"`);
        }
        try {
            await migrateSystemDatabase(this.systemPoolConfig);
        }
        catch (e) {
            const tableExists = await this.pool.query(`SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_schema = 'dbos' AND table_name = 'operation_outputs')`);
            if (tableExists.rows[0].exists) {
                this.logger.warn(`System database migration failed, you may be running an old version of DBOS Transact: ${e.message}`);
            }
            else {
                throw e;
            }
        }
        finally {
            await pgSystemClient.end();
        }
        await this.listenForNotifications();
    }
    async destroy() {
        await this.knexDB.destroy();
        if (this.notificationsClient) {
            this.notificationsClient.removeAllListeners();
            this.notificationsClient.release();
        }
        await this.pool.end();
    }
    async checkWorkflowOutput(workflowUUID) {
        const { rows } = await this.pool.query(`SELECT status, output, error FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_status WHERE workflow_uuid=$1`, [workflowUUID]);
        if (rows.length === 0 || rows[0].status === workflow_1.StatusString.PENDING) {
            return dbos_executor_1.dbosNull;
        }
        else if (rows[0].status === workflow_1.StatusString.ERROR) {
            throw (0, serialize_error_1.deserializeError)(utils_1.DBOSJSON.parse(rows[0].error));
        }
        else {
            return utils_1.DBOSJSON.parse(rows[0].output);
        }
    }
    async initWorkflowStatus(initStatus, args) {
        const result = await this.pool.query(`INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_status (
        workflow_uuid,
        status,
        name,
        class_name,
        config_name,
        queue_name,
        authenticated_user,
        assumed_role,
        authenticated_roles,
        request,
        output,
        executor_id,
        application_version,
        application_id,
        created_at
      ) VALUES($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
       ON CONFLICT (workflow_uuid)
        DO UPDATE SET
          recovery_attempts = CASE WHEN $16 THEN workflow_status.recovery_attempts + 1 ELSE workflow_status.recovery_attempts END
        RETURNING recovery_attempts`, [
            initStatus.workflowUUID,
            initStatus.status,
            initStatus.name,
            initStatus.className,
            initStatus.configName,
            initStatus.queueName,
            initStatus.authenticatedUser,
            initStatus.assumedRole,
            utils_1.DBOSJSON.stringify(initStatus.authenticatedRoles),
            utils_1.DBOSJSON.stringify(initStatus.request),
            null,
            initStatus.executorID,
            initStatus.applicationVersion,
            initStatus.applicationID,
            initStatus.createdAt,
            initStatus.recovery,
        ]);
        const recovery_attempts = result.rows[0].recovery_attempts;
        if (recovery_attempts >= initStatus.maxRetries && initStatus.recovery) {
            await this.pool.query(`UPDATE ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_status SET status=$1 WHERE workflow_uuid=$2 AND status=$3`, [workflow_1.StatusString.RETRIES_EXCEEDED, initStatus.workflowUUID, workflow_1.StatusString.PENDING]);
            throw new error_1.DBOSDeadLetterQueueError(initStatus.workflowUUID, initStatus.maxRetries);
        }
        const { rows } = await this.pool.query(`INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_inputs (workflow_uuid, inputs) VALUES($1, $2) ON CONFLICT (workflow_uuid) DO UPDATE SET workflow_uuid = excluded.workflow_uuid  RETURNING inputs`, [initStatus.workflowUUID, utils_1.DBOSJSON.stringify(args)]);
        return utils_1.DBOSJSON.parse(rows[0].inputs);
    }
    bufferWorkflowOutput(workflowUUID, status) {
        this.workflowStatusBuffer.set(workflowUUID, status);
    }
    /**
     * Flush the workflow output buffer and the input buffer to the database.
     */
    async flushWorkflowSystemBuffers() {
        // Always flush the status buffer first because of foreign key constraints
        await this.flushWorkflowStatusBuffer();
        await this.flushWorkflowInputsBuffer();
    }
    async flushWorkflowStatusBuffer() {
        const localBuffer = new Map(this.workflowStatusBuffer);
        this.workflowStatusBuffer.clear();
        const totalSize = localBuffer.size;
        try {
            let finishedCnt = 0;
            while (finishedCnt < totalSize) {
                let sqlStmt = `INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_status (workflow_uuid, status, name, authenticated_user, assumed_role, authenticated_roles, request, output, executor_id, application_version, application_id, created_at, updated_at, class_name, config_name, queue_name) VALUES `;
                let paramCnt = 1;
                const values = [];
                const batchUUIDs = [];
                for (const [workflowUUID, status] of localBuffer) {
                    if (paramCnt > 1) {
                        sqlStmt += ", ";
                    }
                    sqlStmt += `($${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++}, $${paramCnt++})`;
                    values.push(workflowUUID, status.status, status.name, status.authenticatedUser, status.assumedRole, utils_1.DBOSJSON.stringify(status.authenticatedRoles), utils_1.DBOSJSON.stringify(status.request), utils_1.DBOSJSON.stringify(status.output), status.executorID, status.applicationVersion, status.applicationID, status.createdAt, Date.now(), status.className, status.configName, status.queueName);
                    batchUUIDs.push(workflowUUID);
                    finishedCnt++;
                    if (batchUUIDs.length >= this.flushBatchSize) {
                        // Cap at the batch size.
                        break;
                    }
                }
                sqlStmt += " ON CONFLICT (workflow_uuid) DO UPDATE SET status=EXCLUDED.status, output=EXCLUDED.output, updated_at=EXCLUDED.updated_at;";
                await this.pool.query(sqlStmt, values);
                // Clean up after each batch succeeds
                batchUUIDs.forEach((value) => {
                    localBuffer.delete(value);
                });
            }
        }
        catch (error) {
            error.message = `Error flushing workflow status buffer: ${error.message}`;
            this.logger.error(error);
        }
        finally {
            // If there are still items in flushing the buffer, return items to the global buffer for retrying later.
            for (const [workflowUUID, output] of localBuffer) {
                if (!this.workflowStatusBuffer.has(workflowUUID)) {
                    this.workflowStatusBuffer.set(workflowUUID, output);
                }
            }
        }
        return;
    }
    async recordWorkflowError(workflowUUID, status) {
        await this.pool.query(`INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_status (
        workflow_uuid,
        status,
        name,
        class_name,
        config_name,
        queue_name,
        authenticated_user,
        assumed_role,
        authenticated_roles,
        request,
        error,
        executor_id,
        application_id,
        application_version,
        created_at,
        updated_at
    ) VALUES($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16)
    ON CONFLICT (workflow_uuid)
    DO UPDATE SET status=EXCLUDED.status, error=EXCLUDED.error, updated_at=EXCLUDED.updated_at;`, [
            workflowUUID,
            workflow_1.StatusString.ERROR,
            status.name,
            status.className,
            status.configName,
            status.queueName,
            status.authenticatedUser,
            status.assumedRole,
            utils_1.DBOSJSON.stringify(status.authenticatedRoles),
            utils_1.DBOSJSON.stringify(status.request),
            status.error,
            status.executorID,
            status.applicationID,
            status.applicationVersion,
            status.createdAt,
            Date.now(),
        ]);
    }
    async getPendingWorkflows(executorID) {
        const { rows } = await this.pool.query(`SELECT workflow_uuid FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_status WHERE status=$1 AND executor_id=$2`, [workflow_1.StatusString.PENDING, executorID]);
        return rows.map(i => i.workflow_uuid);
    }
    bufferWorkflowInputs(workflowUUID, args) {
        this.workflowInputsBuffer.set(workflowUUID, args);
    }
    async flushWorkflowInputsBuffer() {
        const localBuffer = new Map(this.workflowInputsBuffer);
        this.workflowInputsBuffer.clear();
        const totalSize = localBuffer.size;
        try {
            let finishedCnt = 0;
            while (finishedCnt < totalSize) {
                let sqlStmt = `INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_inputs (workflow_uuid, inputs) VALUES `;
                let paramCnt = 1;
                const values = [];
                const batchUUIDs = [];
                for (const [workflowUUID, args] of localBuffer) {
                    finishedCnt++;
                    if (this.workflowStatusBuffer.has(workflowUUID)) {
                        // Need the workflow status buffer to be flushed first. Continue and retry later.
                        continue;
                    }
                    if (paramCnt > 1) {
                        sqlStmt += ", ";
                    }
                    sqlStmt += `($${paramCnt++}, $${paramCnt++})`;
                    values.push(workflowUUID, utils_1.DBOSJSON.stringify(args));
                    batchUUIDs.push(workflowUUID);
                    if (batchUUIDs.length >= this.flushBatchSize) {
                        // Cap at the batch size.
                        break;
                    }
                }
                if (batchUUIDs.length > 0) {
                    sqlStmt += " ON CONFLICT (workflow_uuid) DO NOTHING;";
                    await this.pool.query(sqlStmt, values);
                    // Clean up after each batch succeeds
                    batchUUIDs.forEach((value) => { localBuffer.delete(value); });
                }
            }
        }
        catch (error) {
            error.message = `Error flushing workflow inputs buffer: ${error.message}`;
            this.logger.error(error);
        }
        finally {
            // If there are still items in flushing the buffer, return items to the global buffer for retrying later.
            for (const [workflowUUID, args] of localBuffer) {
                if (!this.workflowInputsBuffer.has(workflowUUID)) {
                    this.workflowInputsBuffer.set(workflowUUID, args);
                }
            }
        }
        return;
    }
    async getWorkflowInputs(workflowUUID) {
        const { rows } = await this.pool.query(`SELECT inputs FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_inputs WHERE workflow_uuid=$1`, [workflowUUID]);
        if (rows.length === 0) {
            return null;
        }
        return utils_1.DBOSJSON.parse(rows[0].inputs);
    }
    async checkOperationOutput(workflowUUID, functionID) {
        const { rows } = await this.pool.query(`SELECT output, error FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.operation_outputs WHERE workflow_uuid=$1 AND function_id=$2`, [workflowUUID, functionID]);
        if (rows.length === 0) {
            return dbos_executor_1.dbosNull;
        }
        else if (utils_1.DBOSJSON.parse(rows[0].error) !== null) {
            throw (0, serialize_error_1.deserializeError)(utils_1.DBOSJSON.parse(rows[0].error));
        }
        else {
            return utils_1.DBOSJSON.parse(rows[0].output);
        }
    }
    async recordOperationOutput(workflowUUID, functionID, output) {
        const serialOutput = utils_1.DBOSJSON.stringify(output);
        try {
            await this.pool.query(`INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.operation_outputs (workflow_uuid, function_id, output) VALUES ($1, $2, $3);`, [workflowUUID, functionID, serialOutput]);
        }
        catch (error) {
            const err = error;
            if (err.code === "40001" || err.code === "23505") {
                // Serialization and primary key conflict (Postgres).
                throw new error_1.DBOSWorkflowConflictUUIDError(workflowUUID);
            }
            else {
                throw err;
            }
        }
    }
    async recordOperationError(workflowUUID, functionID, error) {
        const serialErr = utils_1.DBOSJSON.stringify((0, serialize_error_1.serializeError)(error));
        try {
            await this.pool.query(`INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.operation_outputs (workflow_uuid, function_id, error) VALUES ($1, $2, $3);`, [workflowUUID, functionID, serialErr]);
        }
        catch (error) {
            const err = error;
            if (err.code === "40001" || err.code === "23505") {
                // Serialization and primary key conflict (Postgres).
                throw new error_1.DBOSWorkflowConflictUUIDError(workflowUUID);
            }
            else {
                throw err;
            }
        }
    }
    /**
     *  Guard the operation, throwing an error if a conflicting execution is detected.
     */
    async recordNotificationOutput(client, workflowUUID, functionID, output) {
        try {
            await client.query(`INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.operation_outputs (workflow_uuid, function_id, output) VALUES ($1, $2, $3);`, [workflowUUID, functionID, utils_1.DBOSJSON.stringify(output)]);
        }
        catch (error) {
            await client.query("ROLLBACK");
            client.release();
            const err = error;
            if (err.code === "40001" || err.code === "23505") {
                // Serialization and primary key conflict (Postgres).
                throw new error_1.DBOSWorkflowConflictUUIDError(workflowUUID);
            }
            else {
                throw err;
            }
        }
    }
    async sleepms(workflowUUID, functionID, durationMS) {
        const { rows } = await this.pool.query(`SELECT output FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.operation_outputs WHERE workflow_uuid=$1 AND function_id=$2`, [workflowUUID, functionID]);
        if (rows.length > 0) {
            const endTimeMs = utils_1.DBOSJSON.parse(rows[0].output);
            await (0, utils_1.sleepms)(Math.max(endTimeMs - Date.now(), 0));
            return;
        }
        else {
            const endTimeMs = Date.now() + durationMS;
            await this.pool.query(`INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.operation_outputs (workflow_uuid, function_id, output) VALUES ($1, $2, $3) ON CONFLICT DO NOTHING;`, [workflowUUID, functionID, utils_1.DBOSJSON.stringify(endTimeMs)]);
            await (0, utils_1.sleepms)(Math.max(endTimeMs - Date.now(), 0));
            return;
        }
    }
    nullTopic = "__null__topic__";
    async send(workflowUUID, functionID, destinationUUID, message, topic) {
        topic = topic ?? this.nullTopic;
        const client = await this.pool.connect();
        await client.query("BEGIN ISOLATION LEVEL READ COMMITTED");
        const { rows } = await client.query(`SELECT output FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.operation_outputs WHERE workflow_uuid=$1 AND function_id=$2`, [workflowUUID, functionID]);
        if (rows.length > 0) {
            await client.query("ROLLBACK");
            client.release();
            return;
        }
        try {
            await client.query(`INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.notifications (destination_uuid, topic, message) VALUES ($1, $2, $3);`, [destinationUUID, topic, utils_1.DBOSJSON.stringify(message)]);
        }
        catch (error) {
            await client.query("ROLLBACK");
            client.release();
            const err = error;
            if (err.code === "23503") {
                // Foreign key constraint violation
                throw new error_1.DBOSNonExistentWorkflowError(`Sent to non-existent destination workflow UUID: ${destinationUUID}`);
            }
            else {
                throw err;
            }
        }
        await this.recordNotificationOutput(client, workflowUUID, functionID, undefined);
        await client.query("COMMIT");
        client.release();
    }
    async recv(workflowUUID, functionID, timeoutFunctionID, topic, timeoutSeconds = dbos_executor_1.DBOSExecutor.defaultNotificationTimeoutSec) {
        topic = topic ?? this.nullTopic;
        // First, check for previous executions.
        const checkRows = (await this.pool.query(`SELECT output FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.operation_outputs WHERE workflow_uuid=$1 AND function_id=$2`, [workflowUUID, functionID])).rows;
        if (checkRows.length > 0) {
            return utils_1.DBOSJSON.parse(checkRows[0].output);
        }
        // Check if the key is already in the DB, then wait for the notification if it isn't.
        const initRecvRows = (await this.pool.query(`SELECT topic FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.notifications WHERE destination_uuid=$1 AND topic=$2;`, [workflowUUID, topic])).rows;
        if (initRecvRows.length === 0) {
            // Then, register the key with the global notifications listener.
            let resolveNotification;
            const messagePromise = new Promise((resolve) => {
                resolveNotification = resolve;
            });
            const payload = `${workflowUUID}::${topic}`;
            this.notificationsMap[payload] = resolveNotification; // The resolver assignment in the Promise definition runs synchronously.
            let timer;
            const timeoutPromise = new Promise(async (resolve, reject) => {
                try {
                    await this.sleepms(workflowUUID, timeoutFunctionID, timeoutSeconds * 1000);
                    resolve();
                }
                catch (e) {
                    this.logger.error(e);
                    reject(new Error('sleepms failed'));
                }
            });
            try {
                await Promise.race([messagePromise, timeoutPromise]);
            }
            finally {
                clearTimeout(timer);
                delete this.notificationsMap[payload];
            }
        }
        // Transactionally consume and return the message if it's in the DB, otherwise return null.
        const client = await this.pool.connect();
        await client.query(`BEGIN ISOLATION LEVEL READ COMMITTED`);
        const finalRecvRows = (await client.query(`WITH oldest_entry AS (
        SELECT destination_uuid, topic, message, created_at_epoch_ms
        FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.notifications
        WHERE destination_uuid = $1
          AND topic = $2
        ORDER BY created_at_epoch_ms ASC
        LIMIT 1
      )

      DELETE FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.notifications
      USING oldest_entry
      WHERE notifications.destination_uuid = oldest_entry.destination_uuid
        AND notifications.topic = oldest_entry.topic
        AND notifications.created_at_epoch_ms = oldest_entry.created_at_epoch_ms
      RETURNING notifications.*;`, [workflowUUID, topic])).rows;
        let message = null;
        if (finalRecvRows.length > 0) {
            message = utils_1.DBOSJSON.parse(finalRecvRows[0].message);
        }
        await this.recordNotificationOutput(client, workflowUUID, functionID, message);
        await client.query(`COMMIT`);
        client.release();
        return message;
    }
    async setEvent(workflowUUID, functionID, key, message) {
        const client = await this.pool.connect();
        await client.query("BEGIN ISOLATION LEVEL READ COMMITTED");
        let { rows } = await client.query(`SELECT output FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.operation_outputs WHERE workflow_uuid=$1 AND function_id=$2`, [workflowUUID, functionID]);
        if (rows.length > 0) {
            await client.query("ROLLBACK");
            client.release();
            return;
        }
        ({ rows } = await client.query(`INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_events (workflow_uuid, key, value)
       VALUES ($1, $2, $3)
       ON CONFLICT (workflow_uuid, key)
       DO UPDATE SET value = $3
       RETURNING workflow_uuid;`, [workflowUUID, key, utils_1.DBOSJSON.stringify(message)]));
        await this.recordNotificationOutput(client, workflowUUID, functionID, undefined);
        await client.query("COMMIT");
        client.release();
    }
    async getEvent(workflowUUID, key, timeoutSeconds, callerWorkflow) {
        // Check if the operation has been done before for OAOO (only do this inside a workflow).
        if (callerWorkflow) {
            const { rows } = await this.pool.query(`
        SELECT output
        FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.operation_outputs
        WHERE workflow_uuid=$1 AND function_id=$2`, [callerWorkflow.workflowUUID, callerWorkflow.functionID]);
            if (rows.length > 0) {
                return utils_1.DBOSJSON.parse(rows[0].output);
            }
        }
        // Check if the key is already in the DB, then wait for the notification if it isn't.
        const initRecvRows = (await this.pool.query(`
      SELECT key, value
      FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_events
      WHERE workflow_uuid=$1 AND key=$2;`, [workflowUUID, key])).rows;
        // Return the value if it's in the DB, otherwise return null.
        let value = null;
        if (initRecvRows.length > 0) {
            value = utils_1.DBOSJSON.parse(initRecvRows[0].value);
        }
        else {
            // Register the key with the global notifications listener.
            let resolveNotification;
            const valuePromise = new Promise((resolve) => {
                resolveNotification = resolve;
            });
            const payload = `${workflowUUID}::${key}`;
            this.workflowEventsMap[payload] = resolveNotification; // The resolver assignment in the Promise definition runs synchronously.
            let timer;
            const timeoutMillis = timeoutSeconds * 1000;
            const timeoutPromise = callerWorkflow
                ? new Promise(async (resolve, reject) => {
                    try {
                        await this.sleepms(callerWorkflow.workflowUUID, callerWorkflow.timeoutFunctionID, timeoutMillis);
                        resolve();
                    }
                    catch (e) {
                        this.logger.error(e);
                        reject(new Error('sleepms failed'));
                    }
                })
                : new Promise((resolve) => {
                    timer = setTimeout(() => {
                        resolve();
                    }, timeoutMillis);
                });
            try {
                await Promise.race([valuePromise, timeoutPromise]);
            }
            finally {
                clearTimeout(timer);
                delete this.workflowEventsMap[payload];
            }
            const finalRecvRows = (await this.pool.query(`
          SELECT value
          FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_events
          WHERE workflow_uuid=$1 AND key=$2;`, [workflowUUID, key])).rows;
            if (finalRecvRows.length > 0) {
                value = utils_1.DBOSJSON.parse(finalRecvRows[0].value);
            }
        }
        // Record the output if it is inside a workflow.
        if (callerWorkflow) {
            await this.recordOperationOutput(callerWorkflow.workflowUUID, callerWorkflow.functionID, value);
        }
        return value;
    }
    async setWorkflowStatus(workflowUUID, status, resetRecoveryAttempts) {
        await this.pool.query(`UPDATE ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_status SET status=$1 WHERE workflow_uuid=$2`, [status, workflowUUID]);
        if (resetRecoveryAttempts) {
            await this.pool.query(`UPDATE ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_status SET recovery_attempts=0 WHERE workflow_uuid=$1`, [workflowUUID]);
        }
    }
    async getWorkflowStatus(workflowUUID, callerUUID, functionID) {
        // Check if the operation has been done before for OAOO (only do this inside a workflow).
        if (callerUUID !== undefined && functionID !== undefined) {
            const { rows } = await this.pool.query(`SELECT output FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.operation_outputs WHERE workflow_uuid=$1 AND function_id=$2`, [callerUUID, functionID]);
            if (rows.length > 0) {
                return utils_1.DBOSJSON.parse(rows[0].output);
            }
        }
        const { rows } = await this.pool.query(`SELECT status, name, class_name, config_name, authenticated_user, assumed_role, authenticated_roles, request, queue_name FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_status WHERE workflow_uuid=$1`, [workflowUUID]);
        let value = null;
        if (rows.length > 0) {
            value = {
                status: rows[0].status,
                workflowName: rows[0].name,
                workflowClassName: rows[0].class_name || "",
                workflowConfigName: rows[0].config_name || "",
                queueName: rows[0].queue_name || undefined,
                authenticatedUser: rows[0].authenticated_user,
                assumedRole: rows[0].assumed_role,
                authenticatedRoles: utils_1.DBOSJSON.parse(rows[0].authenticated_roles),
                request: utils_1.DBOSJSON.parse(rows[0].request),
            };
        }
        // Record the output if it is inside a workflow.
        if (callerUUID !== undefined && functionID !== undefined) {
            await this.recordOperationOutput(callerUUID, functionID, value);
        }
        return value;
    }
    async getWorkflowResult(workflowUUID) {
        const pollingIntervalMs = 1000;
        while (true) {
            const { rows } = await this.pool.query(`SELECT status, output, error FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_status WHERE workflow_uuid=$1`, [workflowUUID]);
            if (rows.length > 0) {
                const status = rows[0].status;
                if (status === workflow_1.StatusString.SUCCESS) {
                    return utils_1.DBOSJSON.parse(rows[0].output);
                }
                else if (status === workflow_1.StatusString.ERROR) {
                    throw (0, serialize_error_1.deserializeError)(utils_1.DBOSJSON.parse(rows[0].error));
                }
            }
            await (0, utils_1.sleepms)(pollingIntervalMs);
        }
    }
    /* BACKGROUND PROCESSES */
    /**
     * A background process that listens for notifications from Postgres then signals the appropriate
     * workflow listener by resolving its promise.
     */
    async listenForNotifications() {
        this.notificationsClient = await this.pool.connect();
        await this.notificationsClient.query("LISTEN dbos_notifications_channel;");
        await this.notificationsClient.query("LISTEN dbos_workflow_events_channel;");
        const handler = (msg) => {
            if (msg.channel === 'dbos_notifications_channel') {
                if (msg.payload && msg.payload in this.notificationsMap) {
                    this.notificationsMap[msg.payload]();
                }
            }
            else {
                if (msg.payload && msg.payload in this.workflowEventsMap) {
                    this.workflowEventsMap[msg.payload]();
                }
            }
        };
        this.notificationsClient.on("notification", handler);
    }
    // Event dispatcher queries / updates
    async getEventDispatchState(svc, wfn, key) {
        const res = await this.pool.query(`
      SELECT *
      FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.event_dispatch_kv
      WHERE workflow_fn_name = $1
      AND service_name = $2
      AND key = $3;
    `, [wfn, svc, key]);
        if (res.rows.length === 0)
            return undefined;
        return {
            service: res.rows[0].service_name,
            workflowFnName: res.rows[0].workflow_fn_name,
            key: res.rows[0].key,
            value: res.rows[0].value,
            updateTime: res.rows[0].update_time,
            updateSeq: (res.rows[0].update_seq !== null && res.rows[0].update_seq !== undefined ? BigInt(res.rows[0].update_seq) : undefined),
        };
    }
    async queryEventDispatchState(input) {
        let query = this.knexDB(`${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.event_dispatch_kv`);
        if (input.service) {
            query = query.where('service_name', input.service);
        }
        if (input.workflowFnName) {
            query = query.where('workflow_fn_name', input.workflowFnName);
        }
        if (input.key) {
            query = query.where('key', input.key);
        }
        if (input.startTime) {
            query = query.where('update_time', '>=', new Date(input.startTime).getTime());
        }
        if (input.endTime) {
            query = query.where('update_time', '<=', new Date(input.endTime).getTime());
        }
        if (input.startSeq) {
            query = query.where('update_seq', '>=', input.startSeq);
        }
        if (input.endSeq) {
            query = query.where('update_seq', '<=', input.endSeq);
        }
        const rows = await query.select();
        const ers = rows.map((row) => {
            return {
                service: row.service_name,
                workflowFnName: row.workflow_fn_name,
                key: row.key,
                value: row.value,
                updateTime: row.update_time,
                updateSeq: (row.update_seq !== undefined && row.update_seq !== null ? BigInt(row.update_seq) : undefined),
            };
        });
        return ers;
    }
    async upsertEventDispatchState(state) {
        const res = await this.pool.query(`
      INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.event_dispatch_kv (
        service_name, workflow_fn_name, key, value, update_time, update_seq)
      VALUES ($1, $2, $3, $4, $5, $6)
      ON CONFLICT (service_name, workflow_fn_name, key)
      DO UPDATE SET
        update_time = GREATEST(EXCLUDED.update_time, event_dispatch_kv.update_time),
        update_seq =  GREATEST(EXCLUDED.update_seq,  event_dispatch_kv.update_seq),
        value = CASE WHEN (EXCLUDED.update_time > event_dispatch_kv.update_time OR EXCLUDED.update_seq > event_dispatch_kv.update_seq OR
                            (event_dispatch_kv.update_time IS NULL and event_dispatch_kv.update_seq IS NULL))
          THEN EXCLUDED.value ELSE event_dispatch_kv.value END
      RETURNING value, update_time, update_seq;
    `, [
            state.service,
            state.workflowFnName,
            state.key,
            state.value,
            state.updateTime,
            state.updateSeq,
        ]);
        return {
            service: state.service,
            workflowFnName: state.workflowFnName,
            key: state.key,
            value: res.rows[0].value,
            updateTime: res.rows[0].update_time,
            updateSeq: (res.rows[0].update_seq !== undefined && res.rows[0].update_seq !== null ? BigInt(res.rows[0].update_seq) : undefined),
        };
    }
    async getWorkflows(input) {
        let query = this.knexDB(`${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_status`).orderBy('created_at', 'desc');
        if (input.workflowName) {
            query = query.where('name', input.workflowName);
        }
        if (input.authenticatedUser) {
            query = query.where('authenticated_user', input.authenticatedUser);
        }
        if (input.startTime) {
            query = query.where('created_at', '>=', new Date(input.startTime).getTime());
        }
        if (input.endTime) {
            query = query.where('created_at', '<=', new Date(input.endTime).getTime());
        }
        if (input.status) {
            query = query.where('status', input.status);
        }
        if (input.applicationVersion) {
            query = query.where('application_version', input.applicationVersion);
        }
        if (input.limit) {
            query = query.limit(input.limit);
        }
        const rows = await query.select('workflow_uuid');
        const workflowUUIDs = rows.map(row => row.workflow_uuid);
        return {
            workflowUUIDs: workflowUUIDs
        };
    }
    async getWorkflowQueue(input) {
        let query = this.knexDB(`${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_queue`).orderBy('created_at_epoch_ms', 'desc');
        if (input.queueName) {
            query = query.where('queue_name', input.queueName);
        }
        if (input.startTime) {
            query = query.where('created_at_epoch_ms', '>=', new Date(input.startTime).getTime());
        }
        if (input.endTime) {
            query = query.where('created_at_at_epoch_ms', '<=', new Date(input.endTime).getTime());
        }
        if (input.limit) {
            query = query.limit(input.limit);
        }
        const rows = await query.select();
        const workflows = rows.map((row) => {
            return {
                workflowID: row.workflow_uuid,
                queueName: row.queue_name,
                createdAt: row.created_at_epoch_ms,
                startedAt: row.started_at_epoch_ms,
                completedAt: row.completed_at_epoch_ms,
            };
        });
        return { workflows };
    }
    async enqueueWorkflow(workflowId, queue) {
        const _res = await this.pool.query(`
      INSERT INTO ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_queue (workflow_uuid, queue_name)
      VALUES ($1, $2)
      ON CONFLICT (workflow_uuid)
      DO NOTHING;
    `, [workflowId, queue.name]);
    }
    async dequeueWorkflow(workflowId, queue) {
        if (queue.rateLimit) {
            const time = new Date().getTime();
            const _res = await this.pool.query(`
        UPDATE ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_queue
        SET completed_at_epoch_ms = $2
        WHERE workflow_uuid = $1;
      `, [workflowId, time]);
        }
        else {
            const _res = await this.pool.query(`
        DELETE FROM ${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_queue
        WHERE workflow_uuid = $1;
      `, [workflowId]);
        }
    }
    async findAndMarkStartableWorkflows(queue) {
        const startTimeMs = new Date().getTime();
        const limiterPeriodMS = queue.rateLimit ? queue.rateLimit.periodSec * 1000 : 0;
        const claimedIDs = [];
        await this.knexDB.transaction(async (trx) => {
            // If there is a rate limit, compute how many functions have started in its period.
            let numRecentQueries = 0;
            if (queue.rateLimit) {
                const numRecentQueriesS = (await trx(`${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_queue`)
                    .count()
                    .where('queue_name', queue.name)
                    .andWhere('started_at_epoch_ms', '>', startTimeMs - limiterPeriodMS)
                    .first()).count;
                numRecentQueries = parseInt(`${numRecentQueriesS}`);
                if (numRecentQueries >= queue.rateLimit.limitPerPeriod) {
                    return claimedIDs;
                }
            }
            // Select not-yet-completed functions in the queue ordered by the
            //   time at which they were enqueued.
            // If there is a concurrency limit N, select only the N most recent
            //   functions, else select all of them.
            // Started functions count toward concurrency, will be filtered below
            let query = trx(`${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_queue`)
                .whereNull('completed_at_epoch_ms')
                .andWhere('queue_name', queue.name)
                .select();
            query = query.orderBy('created_at_epoch_ms', 'asc');
            if (queue.concurrency !== undefined) {
                query = query.limit(queue.concurrency);
            }
            // From the functions retrieved, get the workflow IDs of the functions
            // that have not yet been started so we can start them.
            const rows = await query.select(['workflow_uuid', 'started_at_epoch_ms']);
            const workflowIDs = rows
                .filter((row) => !row.started_at_epoch_ms)
                .map(row => row.workflow_uuid);
            for (const id of workflowIDs) {
                // If we have a rate limit, stop starting functions when the number
                //   of functions started this period exceeds the limit.
                if (queue.rateLimit && numRecentQueries >= queue.rateLimit.limitPerPeriod) {
                    break;
                }
                const res = await trx(`${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_status`)
                    .where('workflow_uuid', id)
                    .andWhere('status', workflow_1.StatusString.ENQUEUED)
                    .update('status', workflow_1.StatusString.PENDING);
                if (res > 0) {
                    claimedIDs.push(id);
                    await trx(`${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_queue`)
                        .where('workflow_uuid', id)
                        .update('started_at_epoch_ms', startTimeMs);
                }
                // If we did not update this record, probably someone else did.  Count in either case.
                ++numRecentQueries;
            }
        }, { isolationLevel: "repeatable read" });
        // If we have a rate limit, garbage-collect all completed functions started
        //   before the period. If there's no limiter, there's no need--they were
        //   deleted on completion.
        if (queue.rateLimit) {
            await this.knexDB(`${dbos_executor_1.DBOSExecutor.systemDBSchemaName}.workflow_queue`)
                .whereNotNull('completed_at_epoch_ms')
                .andWhere('queue_name', queue.name)
                .andWhere('started_at_epoch_ms', '<', startTimeMs - limiterPeriodMS)
                .delete();
        }
        // Return the IDs of all functions we marked started
        return claimedIDs;
    }
}
exports.PostgresSystemDatabase = PostgresSystemDatabase;
//# sourceMappingURL=system_database.js.map